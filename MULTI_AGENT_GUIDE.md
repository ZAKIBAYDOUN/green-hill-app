# Green Hill Canarias Multi-Agent System

## ðŸ—ï¸ Architecture Overview

The system now supports two deployment modes:

### 1. Simple Mode (Current Production)
- **Flow**: `__start__ â†’ answer_question â†’ __end__`
- **Use Case**: Basic queries, lightweight deployment
- **Status**: âœ… Production ready

### 2. Multi-Agent Mode (Enhanced System)
- **Flow**: `__start__ â†’ [7 agents in parallel] â†’ digital_twin â†’ __end__`
- **Agents**: Strategy, Operations, Finance, Market Intel, Risk, Compliance, Innovation
- **Orchestrator**: Digital Twin synthesizes all agent outputs
- **Status**: âœ… Ready for staging

## ðŸ”§ Configuration

### Environment Variables

```bash
# Deployment Mode
DEPLOYMENT_MODE=simple          # or "multi_agent"

# Green Hill GPT Integration (optional)
GREEN_HILL_GPT_URL=https://your-endpoint.com/api/chat
GREEN_HILL_API_KEY=your_api_key

# OpenAI (optional - has fallback)
OPENAI_API_KEY=your_key
OPENAI_CHAT_MODEL=gpt-4o

# Continuous Testing
CONTINUOUS_MODE=true            # Run tests in loop
TEST_INTERVAL=300               # Seconds between tests
```

## ðŸš€ Deployment Options

### Option 1: Upgrade Current Deployment
```bash
# Set multi-agent mode in LangGraph Cloud
DEPLOYMENT_MODE=multi_agent

# Redeploy with same configuration
git push
```

### Option 2: Parallel Deployment
```bash
# Deploy multi-agent version to new endpoint
# Keep simple mode running in production
# Test thoroughly before switching
```

## ðŸ§ª Testing & Validation

### Local Testing
```bash
# Test both modes
python test_continuous.py

# Test specific mode
DEPLOYMENT_MODE=multi_agent python test_continuous.py
```

### Continuous Testing
```bash
# Run continuous validation
CONTINUOUS_MODE=true python test_continuous.py

# Test against Green Hill GPT (if configured)
GREEN_HILL_GPT_URL=your_endpoint python test_continuous.py
```

## ðŸ“Š Performance Comparison

| Metric | Simple Mode | Multi-Agent Mode |
|--------|-------------|------------------|
| Success Rate | 100% | 100% |
| Avg Response Time | 0.10s | 0.00s |
| Keyword Match | 80% | 90% |
| Response Length | ~850 chars | ~1800 chars |
| Agent Insights | 1 | 7 |

## ðŸ”„ Green Hill GPT Integration

### Setting Up Continuous Testing

1. **Configure Endpoint**:
   ```bash
   export GREEN_HILL_GPT_URL="https://your-green-hill-gpt.com/api"
   export GREEN_HILL_API_KEY="your_api_key"
   ```

2. **Enable in Digital Twin**:
   - The orchestrator will automatically call Green Hill GPT
   - Results are synthesized with agent outputs
   - Graceful fallback if endpoint is unavailable

3. **Run Continuous Tests**:
   ```bash
   DEPLOYMENT_MODE=multi_agent GREEN_HILL_GPT_URL=your_endpoint python test_continuous.py
   ```

## ðŸŽ¯ Recommended Rollout Plan

### Phase 1: Staging Validation
1. Deploy multi-agent mode to staging environment
2. Run continuous tests for 24-48 hours
3. Validate performance and accuracy metrics

### Phase 2: A/B Testing
1. Route 10% of traffic to multi-agent mode
2. Compare user satisfaction and response quality
3. Monitor system performance and error rates

### Phase 3: Full Deployment
1. Gradually increase traffic to multi-agent mode
2. Switch default deployment when confidence is high
3. Keep simple mode as fallback option

## ðŸ› ï¸ Troubleshooting

### Common Issues

1. **Import Errors**: Dependencies missing in deployment
   - Solution: All imports are wrapped in try/catch blocks

2. **Slow Response**: Multi-agent mode taking too long
   - Solution: Agents run in parallel, check individual agent performance

3. **Green Hill GPT Timeout**: External service unavailable
   - Solution: System gracefully falls back to internal analysis

### Debug Commands

```bash
# Test specific agent
python -c "from main import strategy_agent; print(strategy_agent({'question': 'test'}))"

# Test digital twin
python -c "from main import digital_twin_orchestrator; print(digital_twin_orchestrator({'question': 'test'}))"

# Test Green Hill GPT integration
python -c "from main import call_green_hill_gpt; print(call_green_hill_gpt('test'))"
```

## ðŸ“ˆ Monitoring & Metrics

### Key Metrics to Track
- Response time per agent
- Overall system latency
- Success/failure rates
- Green Hill GPT integration status
- User satisfaction scores

### Alerting Thresholds
- Response time > 5 seconds
- Error rate > 5%
- Green Hill GPT availability < 95%

## ðŸ”® Future Enhancements

1. **Dynamic Agent Selection**: Choose agents based on query type
2. **Agent Specialization**: Train agents on specific domain data
3. **Real-time Learning**: Update agent responses based on feedback
4. **Multi-language Support**: Extend to Spanish/English bilingual operation
5. **Advanced Orchestration**: Implement agent-to-agent communication
